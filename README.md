# Project 2: Udacity Sparkify Data Warehouse
### Introduction
In this project, we try to help one music streaming startup, Sparkify, to move their user base and song database processes on to the cloud. Specifically, I build an ETL pipeline that extracts their data from AWS S3 (data storage), stages tables on AWS Redshift (data warehouse with columnar storage), and execute SQL statements that create the schema database from these staging tables.
### Dataset
We will be working with 3 datasets that reside in us-west-2 S3 bucket provided by udacity. Here are the S3 buckets links for each:
- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data
- This third file s3://udacity-dend/log_json_path.json contains the meta information that is required by AWS to correctly load s3://udacity-dend/log_data
#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like.
```json
{
  "num_songs": 1, 
  "artist_id": "ARJIE2Y1187B994AB7", 
  "artist_latitude": null, 
  "artist_longitude": null, 
  "artist_location": "", 
  "artist_name": "Line Renaud", 
  "song_id": "SOUPIRU12A6D4FA1E1", 
  "title": "Der Kleine Dompfaff", 
  "duration": 152.92036, 
  "year": 0
}
```
#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
### Schema
#### Staging tables
We have two staging tables which copy the several JSON files inside the S3 buckets.

- staging_events: the staging table which indicates the actions done by users (which song are listening, etc.. ), include the data from log data

```plantuml
@startuml
entity "staging_events" as staging_events {
    artist: VARCHAR(255),
    auth: VARCHAR(255),
    firstName: VARCHAR(255),
    gender: CHAR(1),
    itemInSession: INTEGER,
    lastName: VARCHAR(255),
    length: FLOAT,
    level: VARCHAR(255),
    location: VARCHAR(255),
    method: VARCHAR(255),
    page: VARCHAR(255),
    registration: FLOAT,
    sessionId: INTEGER,
    song: VARCHAR(255),
    status: INTEGER,
    ts: TIMESTAMP,
    userAgent: VARCHAR(255),
    userId: INTEGER
}
@enduml
```
- staging_songs: the staging table which indicates the information of songs and artists, includes the song data

```plantuml
@startuml
entity "staging_songs" as staging_songs {
    artist_id: VARCHAR(255),
    artist_latitude: FLOAT,
    artist_location: VARCHAR(255),
    artist_longitude: FLOAT,
    artist_name: VARCHAR(255),
    duration: FLOAT,
    num_songs: INTEGER,
    song_id: VARCHAR(255),
    title: VARCHAR(255),
    year: INTEGER
}
@enduml
```
#### Fact table
songplays - records in event data associated with song plays i.e. records with page NextSong

```plantuml
@startuml
entity "songplays" as songplays {
    songplay_id: INTEGER <<auto incremental>> <<DISTKEY>> <<SORTKEY>> <<PK>>,
    start_time: TIMESTAMP NOT NULL <<SORTKEY>> <<FK>>,
    user_id: VARCHAR(255) NOT NULL <<SORTKEY>> <<FK>>,
    song_id: VARCHAR(255) <<SORTKEY>> <<FK>>,
    artist_id VARCHAR(255) <<SORTKEY>> <<FK>>,
    level VARCHAR(255),
    session_id INTEGER,
    location VARCHAR(255),
    user_agent VARCHAR(255)
}
@enduml
```
#### Dimension tables
- users - users in the app
```plantuml
@startuml
entity "users" as users {
    user_id: INTEGER NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>, 
    first_name: VARCHAR(255) NOT NULL,
    last_name: VARCHAR(255) NOT NULL, 
    gender: CHAR(1) NOT NULL, 
    level: VARCHAR(255) NOT NULL
}
@enduml
```
- songs - songs in music database
```plantuml
@startuml
entity "songs" as songs {
     song_id: VARCHAR(255) NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>,
     title: VARCHAR(255) NOT NULL,
     artist_id: VARCHAR(255) NOT NULL,
     year: VARCHAR(255) NOT NULL,
     duration: FLOAT
}
@enduml
```
- artists - artists in music database
```plantuml
@startuml
entity "artists" as artists {
    artist_id: VARCHAR(255) NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>,
    name: VARCHAR(255) NOT NULL,
    location: VARCHAR(255),
    latitude: FLOAT,
    longitude: FLOAT
}
@enduml
```
- time - timestamps of records in songplays broken down into specific units
```plantuml
@startuml
entity "time" as time {
    start_time TIMESTAMP NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>, 
    hour INTEGER NOT NULL, 
    day INTEGER NOT NULL, 
    week INTEGER NOT NULL, 
    month INTEGER NOT NULL, 
    year INTEGER NOT NULL, 
    weekday VARCHAR(20) NOT NULL
}
@enduml
```
### Database Design
``` plantuml
@startuml
entity "staging_events" as staging_events {
    artist: VARCHAR(255),
    auth: VARCHAR(255),
    firstName: VARCHAR(255),
    gender: CHAR(1),
    itemInSession: INTEGER,
    lastName: VARCHAR(255),
    length: FLOAT,
    level: VARCHAR(255),
    location: VARCHAR(255),
    method: VARCHAR(255),
    page: VARCHAR(255),
    registration: FLOAT,
    sessionId: INTEGER,
    song: VARCHAR(255),
    status: INTEGER,
    ts: TIMESTAMP,
    userAgent: VARCHAR(255),
    userId: INTEGER
}

entity "staging_songs" as staging_songs {
    artist_id: VARCHAR(255),
    artist_latitude: FLOAT,
    artist_location: VARCHAR(255),
    artist_longitude: FLOAT,
    artist_name: VARCHAR(255),
    duration: FLOAT,
    num_songs: INTEGER,
    song_id: VARCHAR(255),
    title: VARCHAR(255),
    year: INTEGER
}

entity "users" as users {
    user_id: INTEGER NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>, 
    first_name: VARCHAR(255) NOT NULL,
    last_name: VARCHAR(255) NOT NULL, 
    gender: CHAR(1) NOT NULL, 
    level: VARCHAR(255) NOT NULL
}

entity "songs" as songs {
     song_id: VARCHAR(255) NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>,
     title: VARCHAR(255) NOT NULL,
     artist_id: VARCHAR(255) NOT NULL,
     year: VARCHAR(255) NOT NULL,
     duration: FLOAT
}

entity "artists" as artists {
    artist_id: VARCHAR(255) NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>,
    name: VARCHAR(255) NOT NULL,
    location: VARCHAR(255),
    latitude: FLOAT,
    longitude: FLOAT
}

entity "time" as time {
    start_time TIMESTAMP NOT NULL <<DISTKEY>> <<SORTKEY>> <<PK>>, 
    hour INTEGER NOT NULL, 
    day INTEGER NOT NULL, 
    week INTEGER NOT NULL, 
    month INTEGER NOT NULL, 
    year INTEGER NOT NULL, 
    weekday VARCHAR(20) NOT NULL
}

entity "songplays" as songplays {
    songplay_id: INTEGER <<auto incremental>> <<DISTKEY>> <<SORTKEY>> <<PK>>,
    start_time: TIMESTAMP NOT NULL <<SORTKEY>> <<FK>>,
    user_id: VARCHAR(255) NOT NULL <<SORTKEY>> <<FK>>,
    song_id: VARCHAR(255) <<SORTKEY>> <<FK>>,
    artist_id VARCHAR(255) <<SORTKEY>> <<FK>>,
    level VARCHAR(255),
    session_id INTEGER,
    location VARCHAR(255),
    user_agent VARCHAR(255)
}

songplays }o-- songs
songplays }o-- artists
songplays }|-- users
songplays }|-- time
@enduml 
```
### Project Structure
- `src/create_tables.py` - This script will drop old tables (if exist) ad re-create new tables.
- `src/etl.py` - This script executes the queries that extract `JSON` data from the `S3 bucket` and ingest them to `Redshift`.
- `src/sql_queries.py` - This file contains variables with SQL statement in String formats, partitioned by `CREATE`, `DROP`, `COPY` and `INSERT` statement.
- `src/test.ipynb` - This file contains the execution of etl pipelines.
- `src/configurations/dhw.cfg` - Configuration file in configurations folder used that contains info about `Redshift` cluster, `IAM` role arn of `Redshift` cluster and `S3` bucket provided by udacity
- `src/configurations/credentials.cfg` - Configuration file in configurations folder used that contains all credentials' information (IAM user access key and secret key, redshift username and password).
- `src/configurations/redshift.cfg` - Configuration file in configurations folder used that contains all redshift configurations
- `src/configurations/aws-configurations` - This file contains the running of aws services creation and configurations.
- `src/configurations/environment.py` - This file contain the Configuration class including the necessary for ETL pipeline.
### Setups and Configurations
#### Credentials
- Create a new `IAM user` in your AWS account.
- Attach AdministratorAccess, AmazonRedshiftFullAccess and AmazonS3ReadOnlyAccess policies to the created IAM users.
- Encode the created IAM user access key and secret key to base64 string and fill it to AWS section in credentials.cfg file.
```
[AWS]
KEY=
SECRET=
```
- Create an `IAM Role` that makes `Redshift` able to access `S3 bucket` (ReadOnly)
#### Redshift cluster
##### Prerequisite condition
- Fill in all values in redshift.cfg file, the structure of this file is described below:
```
[REDSHIFT]
CLUSTER_TYPE=
NUM_NODES=
NODE_TYPE=
CLUSTER_IDENTIFIER=
DB_NAME=
PORT=
IAM_ROLE_NAME=
REGION=
```
- Fill in all values in REDSHIFT section in credentials.cfg file to indicate the redshift username and base64 encoded password.
```
[REDSHIFT]
USER=
PASSWORD=
```
##### Create Redshift cluster from code
- Follows the code cell description and run the IaS (infrastructure as code) code cell in aws-configurations.ipynb to create aws services, but do not run the cleanup section until the development or testing are finished.
- After running aws-configurations.ipynb successfully, fill the redshift IAM role ARN, redshift host, redshift dbname, redshift port and redshift region in the values in dwh.cfg:
```
[IAM]
ARN_IAM_ROLE=

[CLUSTER]
HOST=
DB_NAME=
DB_PORT=
REGION=

[S3]
LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'
```
#### Run the ETL pipelines
##### Prerequisite condition
- Loads all configurations from dwh.cfg file
- Fills all sql statements in sql_queries.py based on the database designs above
##### How to run
- Runs create_tables.py to create staging tables, fact tables and dimension tables.
- Runs etl.py to executes the queries that extract `JSON` data from the `S3 bucket` and ingest them to `Redshift`.
- Runs each code cell in test.ipynb steps by steps to verify the data.